{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5ee6a1-f743-4e0c-82ab-d96ab2b08839",
   "metadata": {},
   "source": [
    "## Name: Nikita Bhrugumaharshi Emberi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c6fc5",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# STA 220 Assignment 2\n",
    "\n",
    "Due __Februrary 9, 2024__ by __11:59pm__. Submit your work by uploading it to Gradescope through Canvas.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Provide your solutions in new cells following each exercise description. Create as many new cells as necessary. Use code cells for your Python scripts and Markdown cells for explanatory text or answers to non-coding questions. Answer all textual questions in complete sentences.\n",
    "2. The use of assistive tools is permitted, but must be indicated. You will be graded on you proficiency in coding. Produce high quality code by adhering to proper programming principles. \n",
    "3. Export the .jpynb as .pdf and submit it on Gradescope in time. To facilitate grading, indicate the area of the solution on the submission. Submissions without indication will be marked down. No late submissions accepted. \n",
    "4. If test cases are given, your solution must be in the same format. \n",
    "5. The total number of points is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607f59",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__Exercise 1__\n",
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Sinhala](https://en.wikipedia.org/wiki/Sinhala_language) wikipedia, which is available at [si.wikipedia.org](https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94%E0%B7%80). Additional information of the Sinhala wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Sinhalese, you might want to learn the wiki logic from the english wikipedia, and translate your findings. Also, caching is highly recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ba1fc-7924-4fe3-ad48-2c4ff0de8007",
   "metadata": {},
   "source": [
    "### Importing Required Libraries throughout the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "367d0a6c-11fa-4370-8372-c759465c34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lx\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import re\n",
    "import concurrent.futures, threading\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ae5b02-772c-4000-a008-50a6a8241048",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_cache.install_cache(\"assignment02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276cd17",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94) page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "_How many articles are there?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7eaa2-82db-49e4-97b7-5b66bad19cb4",
   "metadata": {},
   "source": [
    "#### URL of sinhala wikipedia's AllPages page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e432ef0-22c3-4016-98f3-67ac88047e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c19e31-664c-4f9b-aa80-53a19488c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a GET request to the specified URL\n",
    "result = requests.get(url)\n",
    "\n",
    "# Raising an exception if the request isn't successful\n",
    "result.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a073d2b3-5c05-440c-a35b-f7a8a01651fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Chrome webdriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Setting a page load timeout of 20 seconds\n",
    "driver.set_page_load_timeout(20) # twenty seconds should be enough\n",
    "\n",
    "try:\n",
    "    # Attempt to load the URL in the browser\n",
    "    driver.get(url)\n",
    "except TimeoutException:\n",
    "    # If the page load times out, stop loading the page\n",
    "    driver.execute_script(\"window.stop();\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca0eb9-3303-490b-ac61-57934cfcf052",
   "metadata": {},
   "source": [
    "### Final Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888e7b5e-9251-41b1-8ddd-c59104a1c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after skipping Redirections are: 24233\n"
     ]
    }
   ],
   "source": [
    "# Initializing an empty list to store page URLs\n",
    "pages = []\n",
    "\n",
    "# loop until we reach the last page of AllPages site\n",
    "while True: \n",
    "    try:\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # Parsing the current page's HTML source\n",
    "        html = lx.fromstring(driver.page_source)\n",
    "\n",
    "        # Extracting the <ul> element containing page links\n",
    "        ul_element = html.xpath('//div[@class=\"mw-allpages-body\"]/ul[@class=\"mw-allpages-chunk\"]')[0]\n",
    "\n",
    "        # Extracting links from <li> items excluding redirections\n",
    "        li_items = ul_element.xpath('./li[not(@class=\"allpagesredirect\")]/a/@href')\n",
    "\n",
    "        # Extending the list of pages with the extracted links\n",
    "        pages.extend(li_items)    \n",
    "        \n",
    "        div = driver.find_element(\"xpath\", '//*[@id=\"mw-content-text\"]/div[4]')\n",
    "        \n",
    "        # Finding the <a> tag to click on it to go to the next page\n",
    "        a_tags = div.find_elements(\"xpath\", './/a[contains(text(), \"මීළඟ පිටුව\")]')[0]\n",
    "        a_tags.click()\n",
    "\n",
    "    except: \n",
    "        break\n",
    "\n",
    "print(f\"Number of articles after skipping Redirections are: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b58588-a323-451b-bad3-203d61fe87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the Chrome webdriver instance after storing the required URLs\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4407b-8262-4c62-8a0c-e9286c84a282",
   "metadata": {},
   "source": [
    "### Saving the results got so far in the file, because when kernel dies while running multi-threading, I can retrieve the results from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d788a1bc-345d-4b25-a572-69b35662a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving list using pickle serialization\n",
    "def save_list_with_pickle(lst, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(lst, file)\n",
    "\n",
    "# Example usage\n",
    "save_list_with_pickle(pages, 'pages_3.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcbcfa3-3580-4e70-b36c-783338d98409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/%22%E0%B6%91%E0%B6%B8%E0%B7%8A%E0%B6%B4%E0%B6%BA%E0%B6%BB%E0%B7%8A_%E0%B7%83%E0%B7%8A%E0%B6%A7%E0%B7%9A%E0%B6%A7%E0%B7%8A%22_%E0%B6%BA%E0%B6%B1_%E0%B6%B1%E0%B6%B8%E0%B7%99%E0%B7%84%E0%B7%92_%E0%B7%83%E0%B6%B8%E0%B7%8A%E0%B6%B7%E0%B7%80%E0%B6%BA\n"
     ]
    }
   ],
   "source": [
    "# Load list using pickle\n",
    "def load_list_with_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Example usage\n",
    "pages = load_list_with_pickle('pages_3.pickle')\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90213dd",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    "__(iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_\n",
    "\n",
    "\n",
    "_How many links to other articles are there?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d246fd4-56e9-4b9b-a454-b0fa17c5d162",
   "metadata": {},
   "source": [
    "#### Assigning an ID to each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4509854-7883-46e0-9ca3-a2f0e518372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate pages, if there are any\n",
    "pages_set = list(set(pages))\n",
    "\n",
    "pages_with_id = {}\n",
    "\n",
    "# Iterating through unique pages and assigning IDs\n",
    "for id, page in enumerate(pages_set):\n",
    "    pages_with_id[page] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698ca602-c24b-4cc4-9940-a5d1ff3f4be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723e6ff-2e5b-4ecd-bb41-df8be96a78c9",
   "metadata": {},
   "source": [
    "#### An example of how links are assigned an Id and stored in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe30a59-811d-4736-bada-ca1c0d2b7a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0 ----> /wiki/%E0%B6%85%E0%B7%81%E0%B7%8A%E0%B7%80_%E0%B6%BD%E0%B7%8F%E0%B6%A9%E0%B6%B8\n",
      "ID: 1 ----> /wiki/%E0%B7%84%E0%B7%99%E0%B6%A2%E0%B6%B8%E0%B6%B1%E0%B7%92%E0%B6%BA\n",
      "ID: 2 ----> /wiki/%E0%B6%B6%E0%B7%94%E0%B6%AF%E0%B7%8A%E0%B6%B0%E0%B7%92%E0%B6%B8%E0%B6%AD%E0%B7%8A_%E0%B6%B1%E0%B7%92%E0%B6%BA%E0%B7%9D%E0%B6%A2%E0%B7%92%E0%B6%AD%E0%B6%BA%E0%B7%8F\n",
      "ID: 3 ----> /wiki/%E0%B6%9A%E0%B7%90%E0%B6%AD%E0%B6%BB%E0%B7%92%E0%B6%B1%E0%B7%8A,_%E0%B7%80%E0%B7%92%E0%B6%BD%E0%B7%92%E0%B6%BA%E0%B7%9A%E0%B6%B1%E0%B7%84%E0%B7%93_%E0%B6%86%E0%B6%AF%E0%B7%92%E0%B6%B4%E0%B7%8F%E0%B6%AF%E0%B7%80%E0%B6%BB%E0%B7%92%E0%B6%BA\n",
      "ID: 4 ----> /wiki/%E0%B6%AD%E0%B7%99%E0%B6%BB%E0%B7%9A%E0%B7%83%E0%B7%8F_%E0%B6%B8%E0%B7%80%E0%B7%94%E0%B6%AD%E0%B7%94%E0%B6%B8%E0%B7%92%E0%B6%BA\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for page, id in pages_with_id.items():\n",
    "    if count < 5:\n",
    "        print(f\"ID: {id} ----> {page}\")\n",
    "        count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2208d77-7425-4459-9deb-6fbb964cf5dd",
   "metadata": {},
   "source": [
    "(b, i) Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5235195-b99f-4292-982e-7115d77a2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(page):\n",
    "    article_links = []\n",
    "\n",
    "    # Constructing the full valid URL\n",
    "    url = \"https://si.wikipedia.org/\" + page\n",
    "\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    try:\n",
    "        # Raising an exception if the request was not successful\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "        \n",
    "    # Parsing the HTML content\n",
    "    html = lx.fromstring(response.text)\n",
    "    \n",
    "    # Finding the div element with the specific ID\n",
    "    body_div = html.xpath('//div[@id=\"bodyContent\"]')\n",
    "    \n",
    "    # Checking if the div element was found\n",
    "    if body_div:\n",
    "        # Selecting all <a> tags inside the div element\n",
    "        links = body_div[0].xpath('.//a/@href')\n",
    "        for link in links:\n",
    "            # Filtering out links that seem to fall under the Category of special pages, images, Discussions, edits, etc.\n",
    "            # (?<!org): Removes links containing \"org\" (for external URLs).\n",
    "            # \\/wiki\\/: Checks \"/wiki/\" substring.\n",
    "            # (?!.*:): Removes links with a colon (for non-article URLs).\n",
    "            # [^#]: Matches any character except \"#\".\n",
    "            if re.search(r'(?<!org)\\/wiki\\/(?!.*:)', link):\n",
    "                link = re.sub(r'#.*$', '', link)\n",
    "                article_links.append(link)\n",
    "                \n",
    "        return list(set(article_links))\n",
    "            \n",
    "    else:\n",
    "        print(\"Div element with id 'bodyContent' not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2f2c6-b21c-4d59-b1dc-42df88a511a0",
   "metadata": {},
   "source": [
    "__(b ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9c19ffe-4dba-4b2f-84f1-77537f28cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_redirections(link, session):\n",
    "    # Spliting the link to extract the string after '/wiki/'\n",
    "    parts = link.split('/wiki/')\n",
    "    string_after_wiki = parts[1]\n",
    "\n",
    "    # Constructing the URL to check for redirection\n",
    "    url = f\"https://si.wikipedia.org/w/index.php?title={string_after_wiki}&redirect=no\"\n",
    "\n",
    "    # Sending a GET request to the URL\n",
    "    response = session.get(url)\n",
    "\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    # Parsing the HTML content\n",
    "    html = lx.fromstring(response.text)\n",
    "    try:\n",
    "        # Finding the original page link in case of redirection\n",
    "        original_page = html.xpath('//ul[@class=\"redirectText\"]|//span[@class=\"mw-redirectedfrom\"]')\n",
    "        original_page_link = original_page[0].xpath('.//a/@href')\n",
    "        link = original_page_link[0]\n",
    "    except:\n",
    "        # Returning the original link if no redirection is detected\n",
    "        return link\n",
    "\n",
    "    # Checking if the link is valid based on the requirements specified in (b,i)\n",
    "    if re.search(r'(?<!org)\\/wiki\\/(?!.*:)', link):\n",
    "        link = re.sub(r'#.*$', '', link)\n",
    "        return link\n",
    "        \n",
    "    # Returning None if the link doesn't match the pattern   \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1707f3-ff12-455c-a2cd-937c8397dee2",
   "metadata": {},
   "source": [
    "__(b iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41854eec-0b47-458d-b7e7-c655e7080ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_local = threading.local()\n",
    "\n",
    "def get_session():\n",
    "    '''Create a new requests.Session if there is none in thread_local'''\n",
    "    if not hasattr(thread_local, \"session\"): \n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a3189-1686-4199-a3e2-2d660be3ad19",
   "metadata": {},
   "source": [
    "#### Creating Connection pairs (list of tuples) between an article to other articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84826e1e-9b96-4418-90c4-73fdb0a863fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(page):\n",
    "    session = get_session()\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    # Geting the ID of the specified page from the pages_with_id dictionary\n",
    "    page_id = pages_with_id.get(page)\n",
    "    if page_id is None:\n",
    "        return pairs  # No need to proceed if page ID is not available\n",
    "\n",
    "    # Retrieve all links from the specified page\n",
    "    articles = get_all_links(page)\n",
    "    if articles:\n",
    "        for article in articles:\n",
    "             # Geting the ID of the linked article from the pages_with_id dictionary\n",
    "            article_id = pages_with_id.get(article)\n",
    "\n",
    "            # If the ID of the linked article is not available, then it's a redirection or the article link is not valid\n",
    "            if article_id is None:\n",
    "                # Handling redirections to get the correct article ID\n",
    "                article = handle_redirections(article, session)\n",
    "                article_id = pages_with_id.get(article)\n",
    "\n",
    "            # If the ID of the linked article is available and it's not the same as the current page ID\n",
    "            if article_id is not None and page_id != article_id:\n",
    "                pairs.append((page_id, article_id))\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce8eae-bc80-4491-86c4-e1916aba8ed7",
   "metadata": {},
   "source": [
    "#### Function to perform concurrent threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5072abf9-f370-4309-92c2-7a972f3e196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_concurrent_threading(pages):\n",
    "    # Defining the total number of tasks\n",
    "    total_tasks = len(pages)\n",
    "    \n",
    "    # Creating a progress bar using tqdm\n",
    "    with tqdm(total=total_tasks, desc=\"Processing pages\") as progress_bar:\n",
    "        # Using ThreadPoolExecutor with tqdm to create a progress bar\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers = 12) as executor:\n",
    "            # Using executor.map() to apply the function to each page\n",
    "            answers = list(tqdm(executor.map(create_pairs, pages), total=total_tasks, desc=\"Processing pages\", position=0))\n",
    "            progress_bar.update(total_tasks)\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c429e1bf-ca11-4e0a-9894-16d9742fe146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|███████████████████| 24233/24233 [21:15<00:00, 18.99it/s]\n",
      "Processing pages: 100%|███████████████████| 24233/24233 [21:16<00:00, 18.99it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = perform_concurrent_threading(pages_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a91b2-46bf-49eb-a89e-a4959a1d9e5f",
   "metadata": {},
   "source": [
    "### Final Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc47d9ff-f249-4350-a1cc-f93b55382f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of links to other articles: 340684\n"
     ]
    }
   ],
   "source": [
    "sum_of_lengths = sum(len(lst) for lst in answers)\n",
    "print(f\"The number of links to other articles: {sum_of_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e365",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c7f68a5-4ee7-45ba-9ab7-957e482a3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store dangling nodes\n",
    "dangling_nodes = []\n",
    "\n",
    "# Initializing row counter\n",
    "row = 0 \n",
    "\n",
    "# I iterate through list of tuples which give position (row and column values) to mark \"1\" in matrix\n",
    "size = len(pages_set)\n",
    "transition_matrix = np.zeros((size, size))\n",
    "for lst in answers:\n",
    "    # This means this particular article has connections to other articles\n",
    "    if len(lst) > 0:\n",
    "        for pairs in lst:\n",
    "            # Calculating the probability for each pair and updating the transition matrix accordingly\n",
    "            transition_matrix[pairs[0]][pairs[1]] = 1/len(lst)\n",
    "    # This means this article had no outgoing links, therefore should be stored in dangling nodes list\n",
    "    else:\n",
    "        dangling_nodes.append(row)\n",
    "    row = row + 1\n",
    "\n",
    "# converting the matrix to csr_matrix\n",
    "transition_matrix = csr_matrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f3c92-db50-407a-b5ee-769e0f11c993",
   "metadata": {},
   "source": [
    "#### Transition Matrix Before Handling Dangling Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf79d937-ff2c-437c-b1f9-c8baa8d336ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(transition_matrix.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9449961-4a01-4560-b9fc-189a8b09295f",
   "metadata": {},
   "source": [
    "#### Transition Matrix After Handling Dangling Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9080843-2b0b-4947-864a-db0de552b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dangling nodes: 5465\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Dangling nodes: {len(dangling_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "160156f0-9688-4480-add0-911028a6e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substituting Entire Row of Dangling Node with mean of the number of articles (1/(number_of_articles))\n",
    "transition_matrix[dangling_nodes, : ] = 1/size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11418217-89a3-4689-b843-07c9158b8b23",
   "metadata": {},
   "source": [
    "### Final Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f4f85fb-1399-4a08-a056-cb34b4d23726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [4.12660422e-05 4.12660422e-05 4.12660422e-05 ... 4.12660422e-05\n",
      "  4.12660422e-05 4.12660422e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(transition_matrix.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68fe99",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2f0b4-4e17-4d1e-9ff4-411de6c259c8",
   "metadata": {},
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1158b-8e97-47ab-8412-0e31953acc24",
   "metadata": {},
   "source": [
    "#### Step 1: Calculating Google Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13e365dd-e117-413f-99db-f42e714c4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.85\n",
    "additional_matrix = csr_matrix(np.full((size, size), 1/size))\n",
    "\n",
    "google_matrix = damping_factor * transition_matrix + (1 - damping_factor) * additional_matrix\n",
    "google_matrix = csr_matrix(google_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e91b4e76-9311-4694-b1a4-3d8ab4961b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24233, 24233)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "955677a9-4c36-491f-b9a8-eefbe75e501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Matrix:\n",
      "[[6.18990633e-06 6.18990633e-06 6.18990633e-06 ... 6.18990633e-06\n",
      "  6.18990633e-06 6.18990633e-06]\n",
      " [6.18990633e-06 6.18990633e-06 6.18990633e-06 ... 6.18990633e-06\n",
      "  6.18990633e-06 6.18990633e-06]\n",
      " [6.18990633e-06 6.18990633e-06 6.18990633e-06 ... 6.18990633e-06\n",
      "  6.18990633e-06 6.18990633e-06]\n",
      " ...\n",
      " [4.12660422e-05 4.12660422e-05 4.12660422e-05 ... 4.12660422e-05\n",
      "  4.12660422e-05 4.12660422e-05]\n",
      " [6.18990633e-06 6.18990633e-06 6.18990633e-06 ... 6.18990633e-06\n",
      "  6.18990633e-06 6.18990633e-06]\n",
      " [6.18990633e-06 6.18990633e-06 6.18990633e-06 ... 6.18990633e-06\n",
      "  6.18990633e-06 6.18990633e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Google Matrix:\")\n",
    "print(google_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362d292-0f04-44e8-be55-ea45ed4d5bdc",
   "metadata": {},
   "source": [
    "#### Step 2: Calculating Initial Vector with equal entries resembling that initially all pages are considered to have equal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbe1b41b-e851-401e-9a8e-692d59ce0868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24233,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_vector = np.array([1/size] * size)\n",
    "initial_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e8337ec-8206-479f-b529-8635f80f7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vector:\n",
      "[4.12660422e-05 4.12660422e-05 4.12660422e-05 ... 4.12660422e-05\n",
      " 4.12660422e-05 4.12660422e-05]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial Vector:\")\n",
    "print(initial_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129646c-8214-4263-bda4-e86d569d1416",
   "metadata": {},
   "source": [
    "#### Step 3: Calculation Page Ranks over 40 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e12bba08-5d08-4619-8f1f-3cb9a8c81408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [00:14<00:00,  2.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def pagerank(google_matrix, max_iterations=40):\n",
    "    # Number of Proper Articles in wikipedia\n",
    "    n = transition_matrix.shape[0]\n",
    "\n",
    "    # Starting vector with equal entries\n",
    "    initial_vector = np.array([1/n] * n)  \n",
    "\n",
    "    \n",
    "    for _ in tqdm(range(max_iterations)):\n",
    "        '''V(K + 1) = V(K) * G'''\n",
    "        pagerank_vector = initial_vector * google_matrix\n",
    "\n",
    "        '''V(K + 1) = V(K)'''\n",
    "        initial_vector = pagerank_vector\n",
    "\n",
    "    # Calculating the PageRank vector\n",
    "    return pagerank_vector\n",
    "\n",
    "# Final Ranking of the pages after doing 40 iterations\n",
    "pagerank_vector = pagerank(google_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6e713-df98-41be-835d-62180236de88",
   "metadata": {},
   "source": [
    "__(d ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e66c6915-0e5f-4624-9954-ef6727d96ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13584, 13155,  9768, 10950,  6549, 10328, 18458,  3278, 18301,\n",
       "       12592])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting the pageRanks in descending order and getting Top 10 indices so that I can later retrieve article links using \n",
    "# these Indices from pages_with_id Dictionary\n",
    "top_indices = np.argsort(pagerank_vector.flatten())[::-1][:10]\n",
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0898ca7b-2140-4378-8888-a03149b569cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13584: '/wiki/%E0%B7%81%E0%B7%8A%E2%80%8D%E0%B6%BB%E0%B7%93_%E0%B6%BD%E0%B6%82%E0%B6%9A%E0%B7%8F%E0%B7%80',\n",
       " 13155: '/wiki/%E0%B7%80%E0%B7%9A%E0%B6%B6%E0%B7%90%E0%B6%9A%E0%B7%8A_%E0%B6%B8%E0%B7%90%E0%B7%81%E0%B7%92%E0%B6%B1%E0%B7%8A',\n",
       " 9768: '/wiki/%E0%B6%A2%E0%B7%8F%E0%B6%AD%E0%B7%8A%E2%80%8D%E0%B6%BA%E0%B6%B1%E0%B7%8A%E0%B6%AD%E0%B6%BB_%E0%B7%83%E0%B6%B8%E0%B7%8A%E0%B6%B8%E0%B6%AD_%E0%B6%B4%E0%B7%9C%E0%B6%AD%E0%B7%8A_%E0%B6%85%E0%B6%82%E0%B6%9A%E0%B6%BA',\n",
       " 10950: '/wiki/%E0%B7%80%E0%B7%9A%E0%B6%BD%E0%B7%8F_%E0%B6%9A%E0%B6%BD%E0%B7%8F%E0%B6%B4',\n",
       " 6549: '/wiki/%E0%B7%81%E0%B7%8A%E2%80%8D%E0%B6%BB%E0%B7%93_%E0%B6%BD%E0%B6%82%E0%B6%9A%E0%B7%8F%E0%B7%80%E0%B7%9A_%E0%B7%83%E0%B6%B8%E0%B7%8A%E0%B6%B8%E0%B6%AD_%E0%B7%80%E0%B7%9A%E0%B6%BD%E0%B7%8F%E0%B7%80',\n",
       " 10328: '/wiki/%E0%B6%91%E0%B6%9A%E0%B7%8A%E0%B7%83%E0%B6%AD%E0%B7%8A_%E0%B6%BB%E0%B7%8F%E0%B6%A2%E0%B6%B0%E0%B7%8F%E0%B6%B1%E0%B7%92%E0%B6%BA',\n",
       " 18458: '/wiki/%E0%B6%91%E0%B6%9A%E0%B7%8A%E0%B7%83%E0%B6%AD%E0%B7%8A_%E0%B6%A2%E0%B6%B1%E0%B6%B4%E0%B6%AF%E0%B6%BA',\n",
       " 3278: '/wiki/%E0%B6%AF%E0%B7%92%E0%B6%BA%E0%B6%AB%E0%B7%92%E0%B6%BA',\n",
       " 18301: '/wiki/%E0%B6%AF%E0%B7%92%E0%B6%BA%E0%B6%AB%E0%B7%92%E0%B6%BA_(%E0%B6%B6%E0%B7%84%E0%B7%94%E0%B6%BB%E0%B7%94%E0%B6%AD%E0%B7%8A%E0%B7%84%E0%B6%BB%E0%B6%AB%E0%B6%BA)',\n",
       " 12592: '/wiki/%E0%B7%83%E0%B6%B8%E0%B7%8F%E0%B6%BA%E0%B7%9D%E0%B6%AD_%E0%B7%83%E0%B7%8F%E0%B6%BB%E0%B7%8A%E0%B7%80%E0%B6%AD%E0%B7%8A%E2%80%8D%E0%B6%BB_%E0%B7%80%E0%B7%9A%E0%B6%BD%E0%B7%8F%E0%B7%80'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping top 10 indices from above to their respective article links\n",
    "top_10_articles_dict = {}\n",
    "\n",
    "def get_article_url(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None  # If the value is not found in the dictionary\n",
    "\n",
    "for index in top_indices:\n",
    "    key = get_article_url(pages_with_id, index)\n",
    "    top_10_articles_dict[index] = key\n",
    "\n",
    "top_10_articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca6febde-f950-484b-967e-87473c5c32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get English Article Names\n",
    "def english_title(link):\n",
    "    response = requests.get(link)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "        \n",
    "    # Parse the HTML content\n",
    "    html = lx.fromstring(response.text)\n",
    "    \n",
    "    # Find the div element with the specific ID\n",
    "    title = html.xpath('//*[@id=\"firstHeading\"]/span')\n",
    "    return title[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1aeb0f7-022d-4bef-a765-1197b2d5ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map articles links present in Sinhala Wikipedia to English Wikipedia\n",
    "def get_article_name(link):\n",
    "    url = \"https://si.wikipedia.org/\" + link\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "        \n",
    "    # Parsing the HTML content\n",
    "    html = lx.fromstring(response.text)\n",
    "    \n",
    "    # Finding the div element with the specific ID\n",
    "    title = html.xpath('//*[@id=\"firstHeading\"]/span')\n",
    "    \n",
    "    english_title_link = html.xpath('//*[@id=\"p-lang-btn\"]/div/div/ul/li[(@class=\"interlanguage-link interwiki-en mw-list-item\")]/a/@href')\n",
    "    \n",
    "    # Getting the title of the English Wikipedia article if the link is found\n",
    "    if len(english_title_link) > 0:\n",
    "\n",
    "        title = english_title(english_title_link[0])\n",
    "        return title, 1\n",
    "        \n",
    "    return title[0].text, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f05fb6-36c5-4f58-bcae-63660958c215",
   "metadata": {},
   "source": [
    "#### Final Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac28abd0-a1ec-4716-a86b-c3ac7aa2f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top #1 ----> Sri Lanka\n",
      "top #2 ----> Wayback Machine\n",
      "top #3 ----> ISBN\n",
      "top #4 ----> Time zone\n",
      "top #5 ----> Sri Lanka Standard Time\n",
      "top #6 ----> United Kingdom\n",
      "top #7 ----> United States\n",
      "top #8 ----> Daughter\n",
      "top #9 ----> English title not present, Title Name in Sinhala Wikipedia is දියණිය (බහුරුත්හරණය)\n",
      "top #10 ----> Coordinated Universal Time\n"
     ]
    }
   ],
   "source": [
    "# English Names of the top 10 Articles:\n",
    "rank = 0\n",
    "for id, link in top_10_articles_dict.items():\n",
    "    rank = rank + 1\n",
    "    english_titles, found = get_article_name(link)\n",
    "    if found != 0:\n",
    "        print(f\"top #{rank} ----> {english_titles}\")\n",
    "    else:\n",
    "        print(f\"top #{rank} ----> English title not present, Title Name in Sinhala Wikipedia is {english_titles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5ff74-ff21-47cd-8638-10c7e6d92de1",
   "metadata": {},
   "source": [
    "# Acknowledgment\n",
    "I received assistance from `ChatGPT` while working on certain questions in this notebook. I want to clarify that I independently completed the majority of the tasks, seeking help only in instances where I encountered challenges or felt lost. The collaboration with ChatGPT was instrumental in providing guidance and insights during those moments.\n",
    "ChatGpt: https://chat.openai.com/\n",
    "\n",
    "--- Nikita Bhrugumaharshi Emberi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3d5f0-8751-484d-97a4-08b2ca2416a5",
   "metadata": {},
   "source": [
    "## References:\n",
    "1) https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d27d3-ec6d-42bf-a809-920ca8a2155f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
